\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2018

% ready for submission
\usepackage{nips_2018}
\usepackage{algorithm}
\usepackage{algorithmic}
% to compile a preprint version, e.g., for submission to arXiv, add
% add the [preprint] option:
% \usepackage[preprint]{nips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2018}

% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{bm}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb,amsfonts}
\newcommand{\argmax}{\operatornamewithlimits{arg\,max}}
\newcommand{\argmin}{\operatornamewithlimits{arg\,min}}
\newtheorem{assumption}{Assumption}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}

%\usepackage[noend]{algpseudocode}
%\renewcommand{\algorithmicrequire}{\textbf{Input:}} % Use Input in the format of Algorithm
%\renewcommand{\algorithmicensure}{\textbf{Output:}} % Use Output in the format of Algorithm
\title{Max-Discrepant Distributed Learning: Fast Risk Bound and Algorithms}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  %David S.~Hippocampus\thanks{Use footnote for providing further
%    information about author (webpage, alternative
%    address)---\emph{not} for acknowledging funding agencies.} \\
%  Department of Computer Science\\
%  Cranberry-Lemon University\\
%  Pittsburgh, PA 15213 \\
%  \texttt{hippo@cs.cranberry-lemon.edu} \\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}

\end{abstract}
\section{Introduction}
In the era of big data, the rapid expansion of computing capacities in automatic data generation
and acquisition brings data of unprecedented size and complexity, and raises a series
of scientific challenges such as storage bottleneck and algorithmic scalability \cite{zhou2014big,Zhang2013,lin2017distributed}.
%There are some approaches for
%When the size of the dataset becomes extremely large, however, it may be infeasible
%to store all of the data on a single computer, or at least to keep the data in memory.
Distributed learning a feasible method to overcome the difficulty.
The average mixture algorithm perhaps the simplest algorithm for distributed statistical inference.
The algorithm is appealing in its simplicity: partition the dataset $\mathcal{S}$ of size $N$ randomly into $m$ equal sized
subsets $\mathcal{S}_i$, and we compute the estimate for each of the $i=1,\ldots,m$ subsets independently,
and finally compute the average of partition-based estimate.
Theoretical attempts have been recently made in \cite{zhang2012communication,Zhang2013,lin2017distributed}
to derive learning rates for distributed learning.

This paper aims at error analysis of the distributed learning for (regularization) empirical risk minimization.
Given $\mathcal{S}=\left\{z_i=(\mathbf  x_i,y_i)\right\}_{i=1}^N \in (\mathcal{Z}=\mathcal{X}\times \mathcal{Y})^N$,
which is drawn identically and independently from a fixed,
but unknown probability  distribution $\mathbb{P}$ on
$\mathcal{Z}=\mathcal{X}\times\mathcal{Y}$, 
the (regularization) empirical risk minimization can be stated as
\begin{align}
  \hat{f}=\argmin_{f\in\mathcal{H}} \hat{R}(f)=\frac{1}{N}\sum_{j=1}^N\ell(f,z_j)+r(f)
\end{align}
where $\ell(f,z)$ is the loss function, and $r(f)$ is a regularizer.
This learning algorithm  has been well studied in learning theory,
see e.g. \cite{de2005model,caponnetto2007optimal,steinwart2009optimal,smale2007learning,steinwart2008support}.
The distributed learning algorithm studied in this paper starts with partitioning the
data set $\mathcal{S}$ into $m$ disjoint subsets $\{\mathcal{S}_i\}_{i=1}^m$, $|S_i|=\frac{N}{m}=:n$.
Then it assigns each data subset $\mathcal{S}_i$ to one
machine or processor to produce a local estimator $\hat{f}_i$:
\begin{align*}
  \hat{f}_i=\argmin_{f\in\mathcal{H}}\hat{R}_i(f)=
    \frac{1}{|\mathcal{S}_i|}\sum_{z_j\in\mathcal{S}_i}\ell(f,z_j)+r(f).
\end{align*}
%Recall \cite{zhang2012communication}
The finally  global estimator $\bar{f}$ is synthesized by 
$\bar{f}=\frac{1}{m}\sum_{i=1}^m\hat{f}_i.$
This algorithm has been studied with a matrix analysis approach in \cite{zhang2012communication,Zhang2013,lin2017distributed}.
Under local strong convexity, smoothness and a reasonable set of other conditions, \cite{zhang2012communication} show that the combined parameter achieves mean-squared error 
decays as
\begin{align*}
  \mathbb{E}\left[\left\|\bar{f}-f^\ast\right\|_2^2\right]=\mathcal{O}\left(\frac{1}{N}+\left(\frac{N}{m}\right)^2\right),
\end{align*}
where $f^\ast=\argmin_{f\in\mathcal{H}}R(f)=\mathbb{E}_{z\sim \mathbb{P}}[\ell(f,z)]+ r(f).$
\cite{Zhang2013} consider the kernel ridge regression,
under some eigenfunction assumption,
they show that if $m$ is not too large,
\begin{align*}
  \mathbb{E}\left[\left\|\bar{f}-f^\ast\right\|_2^2\right]=\mathcal{O}\left(\|f_\ast\|_\mathcal{H}^2+\frac{\gamma(\lambda)}{N}\right),
\end{align*}
where $\gamma(\lambda)=\sum_{j=1}^\infty\frac{\mu_j}{\lambda+\mu_j}$,
$\mu_j$ is the eigenvalue of a Mercer kernel function.
Without any eigenfunction assumption,
\cite{lin2017distributed} derive a novel bound for some $1\leq p\leq\infty$
\begin{align*}
  \mathbb{E}\left[\left\|\bar{f}-f^\ast\right\|_2\right]=
  \mathcal{O}\left(\left(\frac{\gamma(\lambda)}{N}\right)^{\frac{1}{2}(1-\frac{1}{p})}\left(\frac{1}{N}\right)^{\frac{1}{2p}}\right).
\end{align*}

There are two main contributions.
First, under strongly convex and smooth, and a reasonable set of other conditions,
we derive a risk bound of faster rate:
\begin{align}
\label{theorail-fast-rate}
    R(\bar{f})-R(f_\ast)=\mathcal{O}\left(\frac{R_\ast}{n}
    +\frac{1}{n^2}
    -\Delta(\bar{f})\right).
  \end{align}
  where $R(f)=\mathbb{E}_{z}\left[\ell(f,z)+r(f)\right]$, 
  $\Delta(\bar{f})=\mathcal{O}\left(\frac{1}{m^2}\sum_{i,j=1,i\not=j}^m\|\hat{f}_i-\hat{f}_j\|^2\right)$ is the discrepant
between all partition-based estimates.
 % $\mathcal{N}(\mathcal{H},1/n)$ be the $1/n$-net of $\mathcal{H}$ with minimal cardinality,
%and $C(\mathcal{H},1/n)$ the covering number of $|\mathcal{N}(\mathcal{H},1/n)|$.
When the minimal risk is small, i.e., $R_\ast=\tilde{\mathcal{O}}\left(\frac{1}{n}\right)$,
the rate is improved to
\begin{align*}
    R(\bar{f})-R(f_\ast)=\mathcal{O}\left(\frac{1}{n^2}-\Delta(\bar{f})\right).
\end{align*}
Thus, if $m\leq \sqrt{N}$, the order of $R(\bar{f})-R(f_\ast)$ is faster than 
$
   \mathcal{O}\left(\frac{1}{N}-\Delta(\bar{f})\right).
$

Note that if $\ell(f,z)+r(f)$ is $L$-Lipschitz continuous over $f$,
the order of $R(\bar{f})-R(f^\ast)$ is
\begin{align*}
  R(\bar{f})-R(f^\ast)=\mathcal{O}\left(L\mathbb{E}\left[\left\|\bar{f}-f^\ast\right\|_2\right]\right)=\mathcal{O}\left(L\sqrt{\mathbb{E}\left[\left\|\bar{f}-f^\ast\right\|_2\right]}\right).
\end{align*}
Thus, the order of $R(\bar{f})-R(f^\ast)$ in \cite{zhang2012communication,Zhang2013,lin2017distributed}
 at most $\mathcal{O}\left({\frac{1}{\sqrt{N}}}\right)$, 
 which is much slower than that of our bound of $\mathcal{O}\left(\frac{1}{N}\right)$.
Our second contribution is to develop a novel distributed learning algorithm.
From Equation \eqref{theorail-fast-rate}, we know that to guarantee good risk performance,
the $\Delta(\bar{f})$ should be large.
Therefore, we propose a novel max-discrepant distributed learning criterion:
\begin{align*}
  \hat{f}_i=\argmin_{f\in\mathcal{H}}\frac{1}{|\mathcal{S}_i|}\sum_{z_j\in\mathcal{S}_i}\ell(f,z_j)+r(f)-\gamma \|f-\bar{f}_{\backslash i}\|_\mathcal{H},
\end{align*}
where $\bar{f}_{\backslash i}=\frac{1}{m-1}\sum_{j=1,j\not =i}^m\hat{f}_j$,
the last term is to make $\Delta(\bar{f})$ large.
We present a simple iterative algorithm to solve the above optimization problem.
Experimental results on lots of datasets show that our proposed Max-Discrepant Distributed algorithm (MDD) is sound and efficient.


The rest of the paper is organized as follows.
In Section 2, we derive risk bound of distributed learning with fast rates.
In Section 3, we  propose two novel algorithms based on the max-discrepant of the local estimate.
In Section 4, we analyze the performance of our proposed criterion compared with other state-of-the-art model selection criteria.
We end in Section 5 with conclusion.



%\section{Preliminaries}
%We consider  the supervised learning where a learning algorithm receives a sample of $N$ labeled points
%\begin{align*}
%  \mathcal{S}=\left\{z_i=(\mathbf  x_i,y_i)\right\}_{i=1}^N
%        \in (\mathcal{Z}=\mathcal{X}\times \mathcal{Y})^N,
%\end{align*}
%where $\mathcal{X}$ denotes the input space and
%$\mathcal{Y}$ denotes the output space.
%%$\mathcal{Y}\subset \mathbb{R}$ in the regression case and
%%$\mathcal{Y}=\{-1,+1\}$ in classification case.
%We assume $\mathcal{S}$ is drawn identically and independently from a fixed,
%but unknown probability  distribution $\mathbb{P}$ on
%$\mathcal{Z}=\mathcal{X}\times\mathcal{Y}$.
%The goal is to learn a good prediction model $f\in\mathcal{H}:\mathcal{X}\rightarrow\mathcal{Y}$,
%whose prediction accuracy at instance
%$z=(\mathbf x,y)$ is measured by a loss function $\ell(f,z)$.
%
%
%In this paper, we focus on the (regularization) empirical risk minimization over some Hilbert space $\mathcal{H}$:
%\begin{align}
%  \hat{f}=\argmin_{f\in\mathcal{H}} \hat{R}(f)=\frac{1}{N}\sum_{j=1}^N\ell(f,z_j)+r(f)
%\end{align}
%where $\ell(f,z)$ is the loss function, and $r(f)$ is a regularizer.
%The expect (regularization) risk is defined as
%$$f^\ast=\argmin_{f\in\mathcal{H}}R(f)=\mathbb{E}_{z\sim \mathbb{P}}[\ell(f,z)]+ r(f).$$
%
%In the distributed setting, we divide evenly amongst $m$ processors or inference procedures.
%Let $\mathcal{S}_i, i\in(1,2,\ldots,m)$, denote a subsampled dataset of size $n=\frac{N}{m}$.
%For each $i=1,2,\ldots,m$, the local estimate
%\begin{align*}
%  \hat{f}_i=\argmin_{f\in\mathcal{H}}\hat{R}_i(f)=
%    \frac{1}{n}\sum_{z_j\in\mathcal{S}_i}\ell(f,z_j)+r(f).
%\end{align*}
%The average local estimates is denote as
%$
%  \bar{f}=\frac{1}{m}\sum_{i=1}^m\hat{f}_i.
%$

%In the next, we will estimate the discrepancy of $R(\bar{f})$ and $R(f^\ast)$.
\section{Faster Rates of Distributed Learning}
In this section, we will derive a sharper risk bound under some common assumptions.
\subsection{Assumptions}
In the following, we use $\|\cdot\|_\mathcal{H}$ to denote the norm induced by inner product of the Hilbert space $\mathcal{H}$.
\begin{assumption}
\label{assumption-strongly}
  The function $\nu(f,z)=\ell(f,z)+r(f)$ is $\eta$-strongly convex with respect to the first variable $f$,
  that is $\forall f,f'\in\mathcal{H}$, $z\in\mathcal{Z}$,
  \begin{align}
    \label{assumption-strongly-equation}
     \langle \nabla \nu(f,z), f-f'\rangle_\mathcal{H}+\frac{\eta}{2}\|f-f'\|_\mathcal{H} &\leq \nu(f,z)-\nu(f',z)
  \end{align}
  or (another equivalent definition)
  $\forall  f,f'\in\mathcal{H}, z\in\mathcal{Z}, t\in[0,1]$,
  \begin{align}
  \label{assumption-strongly-second}
  \nu(tf+(1-t)f') \leq  t\nu(f,z)+(1-t)\nu(f',z)-\frac{1}{2}\eta t(t-1)\|f-f'\|_\mathcal{H}^2.
  \end{align}
\end{assumption}
\begin{assumption}
\label{assumption-smooth}
  The function $\nu(f,z)=\ell(f,z)+r(f)$ is $\beta$-smooth with respect to the first variable $f$,
  that is $\forall f,f'\in\mathcal{H}$, $z\in\mathcal{Z}$,
  \begin{align}
     \label{assumption-smooth-equaiton}
     \left\|\nabla\nu(f,z)-\nabla \nu(f',z)\right\|_\mathcal{H}&\leq \beta\|f-f'\|_\mathcal{H}.
  \end{align}
\end{assumption}

\begin{assumption}
\label{assumption-libs}
  The function $\nu(f,z)=\ell(f,z)+r(f)$ is $L$-Lipschitz continuous with respect to the first variable $f$,
  that is $\forall f,f'\in\mathcal{H}$,
  \begin{align}
     \label{assumption-libs-equation}
     \left\|\nu(f,\cdot)- \nu(f',\cdot)\right\|_\mathcal{H}&\leq L\|f-f'\|_\mathcal{H}.
  \end{align}
\end{assumption}

\textbf{Assumptions} \ref{assumption-strongly}, \ref{assumption-smooth} and \ref{assumption-libs} allow us to model some popular losses,
such as square loss and logistic loss, and some regularizer, such as $r(f)=\lambda \|f\|_\mathcal{H}^2$.

\begin{assumption}
\label{assumption-optimal-bound}
  Let $f_\ast=\argmin_{f\in\mathcal{H}}R(f)$. We assume that the gradient at $f_\ast$ is upper bounded by $M$, that is
  \begin{align*}
    \|\nabla \ell(f^\ast,z)\|_\mathcal{H}\leq M, \forall z\in\mathcal{Z}.
  \end{align*}
\end{assumption}
Assumption \ref{assumption-optimal-bound} is also a common assumption, which is used in \cite{Zhang2017er,zhang2012communication}.
\subsection{Faster Rates of Distributed Learning}
Let $\mathcal{N}(\mathcal{H},\epsilon)$ be the $\epsilon$-net of $\mathcal{H}$ with minimal cardinality,
and $C(\mathcal{H},\epsilon)$ the covering number of $|\mathcal{N}(\mathcal{H},\epsilon)|$

\begin{theorem}
\label{theorem-main}
For any $0<\delta<1$, $\epsilon\geq 0$,
under \textbf{Assumptions} \ref{assumption-strongly}, \ref{assumption-smooth}, \ref{assumption-libs} and \ref{assumption-optimal-bound},
and when
  \begin{align}
    \label{equation-12}
    m\leq \frac{N\eta}{4\beta\log C(\mathcal{H},\epsilon)},
  \end{align}
  with probability at least $1-\delta$,
  we have
  \begin{align}
    \label{equation-13}
    \begin{aligned}
    R(\bar{f})-R(f_\ast)&\leq
    \frac{16\beta \log(4m/\delta)}{n^2\eta}+\frac{128\beta R_\ast\log(4m/\delta)}{n\eta}+\frac{32\beta^2\epsilon^2}{\eta}+
    \frac{64\beta L \log C(\mathcal{H},\epsilon)\epsilon}{n\eta}\\
   &~~~~~~\frac{64\beta \log^2C(\mathcal{H},\epsilon)\epsilon^2}{n^2\eta}
   -\Delta(\bar{f}),
  \end{aligned}
  \end{align}
  where $R_\ast=R(f^\ast)$, $\Delta_{\bar{f}}=\frac{\eta}{4m^2}\sum_{i,j=1,i\not=j}^m\|\hat{f}_i-\hat{f}_j\|_\mathcal{H}^2$.
\end{theorem}
From the above theorem, an  interesting finding is that,
when the larger discrepant of each local estimate is,
the tighter the risk bound is.

One can also see that when $\epsilon$ small enough,
$\frac{32\beta^2\epsilon^2}{\eta}+
    \frac{64\beta L \log C(\mathcal{H},\epsilon)\epsilon}{n\eta}
    +\frac{64\beta \log^2C(\mathcal{H},\epsilon)\epsilon^2}{n^2\eta}$
will becomes non-dominating.
To be specific, we have the following corollary:
\begin{corollary}
\label{corollary-first}
  By setting $\epsilon=\frac{1}{n}$ in Theorem \ref{theorem-main},
  when $m\leq \frac{N\eta}{4\beta\log C(\mathcal{H},1/n)}$,
  with high probability,
  we have
  \begin{align*}
    R(\bar{f})-R(f_\ast)=\mathcal{O}\left(\frac{R_\ast\log(m)}{n}
    +\frac{\log(\mathcal{N}(\mathcal{H},\frac{1}{n}))}{n^2}
    -\Delta(\bar{f})\right).
  \end{align*}
\end{corollary}
If the the minimal risk $R(f_\ast)$ is small, i.e., $R(f_\ast)=\mathcal{O}(\frac{1}{n})$,
the rate can even reach $$\mathcal{O}\left(\frac{\log(m)}{n^2}
    +\frac{\log(\mathcal{N}(\mathcal{H},\frac{1}{n}))}{n^2}
    -\Delta(\bar{f})\right).$$
To the best of our knowledge,
this is the first $\tilde{\mathcal{O}}\left(\frac{1}{n^2}\right)$-type of distributed
risk bound of (regularization) empirical risk minimization.

In the next, we will consider two popular Hilbert spaces,
linear space and reproducing kernel Hilbert space for deriving specific risk bounds.
\subsubsection{Linear Space}
\label{subsection-3.1}
The linear hypothesis space is defined as
\begin{align*}
\mathcal{H}=\left\{f=\mathbf w^\mathrm T\mathbf x|\mathbf w\in \mathbb{R}^d, \|\mathbf w\|_2\leq B\right\}.
\end{align*}
According to the \cite{pisier1999volume},
the cover number of linear hypothesis space can be bounded by
\begin{align*}
  \log\left(C(\mathcal{H},\epsilon)\right)\leq d\log \left(6B/\epsilon\right).
\end{align*}
Thus, if we set $\epsilon=\frac{1}{n}$, from Corollary \ref{corollary-first}, we have
\begin{align*}
  R(\bar{f})-R(f_\ast)&=\mathcal{O}\left(\frac{R_\ast\log m}{n}+\frac{d\log n}{n^2}-
  \Delta(\bar{f})\right)
  %&\leq \mathcal{O}\left(\frac{1}{n^2}+\frac{R_\ast\log m}{n}+\frac{d\log n}{n^2}\right)
\end{align*}
When the minimal risk is small, i.e., $R_\ast=\mathcal{O}\left(\frac{d}{n}\right)$,
the rate is improved to
\begin{align*}
    \mathcal{O}\left(\frac{d\log (mn)}{n^2}-\Delta(\bar{f})\right)=\mathcal{O}\left(\frac{d\log N}{n^2}-\Delta(\bar{f})\right).
\end{align*}
Therefore, if $m\leq \sqrt{\frac{N}{d\log N}}$, the order of risk bound can even tighter than
$\mathcal{O}\left(\frac{1}{N}\right).$
\subsubsection{Reproducing Kernel Hilbert Space}
\label{subsection-3.2}
The reproducing kernel Hilbert space $\mathcal{H}_K$ associated with the kernel $K$ is
defined to be the closure of the linear span of the set of functions
$\left\{K(\mathbf x,\cdot):\mathbf x\in\mathcal{X}\right\}$ with the inner product satisfying
\begin{align*}
  \langle K(\mathbf x,\cdot), f\rangle_{\mathcal{H}_K}=f(\mathbf x), \forall \mathbf x\in\mathcal{X}, f\in\mathcal{H}_K.
\end{align*}

The bounded hypothesis space based on the reproducing kernel Hilbert space is defined as
\begin{align*}
  \mathcal{H}:=\left\{f\in\mathcal{H}_{K}: \|f\|_{\mathcal{H}_K}\leq B\right\}.
\end{align*}

From \cite{zhou2002covering}, if the kernel function $K$ is the popular Gaussian kernel over $[0,1]^d$:
$
  K(\mathbf x,\mathbf x')=\exp\left\{-\frac{\|\mathbf x-\mathbf x'\|^2}{\sigma^2}, \mathbf x,\mathbf x' \in[0,1]^d \right\},
$
then for $0\leq \epsilon\leq B/2$, there holds:
$
 \log \left(C(\mathcal{H},1/n)\right)=\mathcal{O}\left(\log^d(nB)\right).$
From Corollary \ref{corollary-first}, if we set $\epsilon=\frac{1}{n}$, and assume $R_\ast=\mathcal{O}\left(\frac{1}{n}\right)$,
we have
\begin{align*}
  R(\bar{f})-R(f_\ast)=\mathcal{O}\left(\frac{\log m}{n^2}+\frac{\log^d n}{n^2}-
  \Delta(\bar{f})\right)
\end{align*}
Therefore, if $m\leq \min\left\{\sqrt{\frac{N}{d\log N}}, \sqrt{\frac{N}{\log^d n}}\right\}$,
the order can tighter than
$\mathcal{O}\left(\frac{1}{N}\right)$.
%Note that for our bound,
%\begin{align*}
%     R(f)-R(f_\ast)=\left\{
%     \begin{aligned}
%     &\mathcal{O}\left(\frac{d\log N}{n^2}-\Delta(\bar{f})\right)  &&\text{linear hypothesis space}\\
%     &\mathcal{O}\left(\frac{\log^d n}{n^2}-\Delta(\bar{f})\right) &&\text{RKHS  space}
%     \end{aligned}
%     \right.
%\end{align*}
%So, we can obtain that
%\begin{align*}
%     R(f)-R(f_\ast)=\mathcal{O}\left(\frac{1}{N}-\Delta(\bar{f})\right)
%     % \left\{
%%     \begin{aligned}
%%     &\mathcal{O}\left(\frac{1}{N}-\Delta(\bar{f})\right)  &\text{if} m\leq \sqrt{\frac{N}{d\log N}} &\text{for linear hypothesis space}\\
%%     &\mathcal{O}\left(\frac{\log^d n}{n^2}-\Delta(\bar{f})\right) &ff &\text{RKHS  space}
%%     \end{aligned}
%%     \right.
%\end{align*}
%if $m\leq \sqrt{\frac{N}{d\log N}}$ for linear hypothesis space, or $m\leq \sqrt{\frac{N}{\log^2 n}}$ for RKHS space.

\subsection{Comparison with Related Work}
%There are few work on the theoretical analysis of average distributed learning.
Under the smooth, strongly convex and other some assumptions,
\cite{zhang2012communication} derive a distributed risk bound:
\begin{align}
  \mathbb{E}\left[\|\bar{f}-f_\ast\|^2\right]=\mathcal{O}\left(\frac{1}{N}+\frac{\log d}{n^2}\right).
\end{align}
%If $\nu(f,z)$ is $L$-Lipschitz continuous over $f$, that is
%\begin{align*}
%  \forall f, f\in \mathcal{H}, z\in\mathcal{Z}, |\nu(f,z)-\nu(f',z)|\leq L\|f-f'\|_\mathcal{H},
%\end{align*}
%it is easy to verity that
%\begin{align}
%  \nonumber R(f)-R(f_\ast)&\leq L\mathbb{E}\left[\|\bar{f}-f_\ast\|_\mathcal{H}\right]\leq L\sqrt{\mathbb{E}\left[\|\bar{f}-f_\ast\|^2_\mathcal{H}\right]}\\
%  \label{related-work-one}    &= \mathcal{O}\left(\frac{1}{\sqrt{N}}+\frac{\sqrt{\log d}}{n}\right).
%\end{align}
%According to the subsections \label{subsection-3.1} and \label{subsection-3.2},
%we know that if $m$ is not very large,
%the order of this paper can reach $\mathcal{O}\left(\frac{1}{N}-\Delta(\bar{f})\right)$,
%which is much sharper than the order of \eqref{related-work-one}.
\cite{Zhang2013} consider the kernel ridge regression,
under some eigenfunction assumption,
they show that if $m$ is not too large,
\begin{align*}
  \mathbb{E}\left[\left\|\bar{f}-f^\ast\right\|^2\right]=\mathcal{O}\left(\frac{r}{N}\right),
\end{align*}
where $r$ is the rank of the kernel function.
Without any eigenfunction assumption,
\cite{lin2017distributed} derive a new bound of $\mathbb{E}\left[\left\|\bar{f}-f^\ast\right\|^2\right]$ of
order at most $\mathcal{O}\left(\frac{1}{N}\right)$.
%and if $m$ is not very large,
%they show that
%\begin{align*}
%   \mathbb{E}\left[\|\bar{f}-f_\ast\|^2\right]=\mathcal{O}\left(\frac{1}{N}\right).
%\end{align*}
If $\nu(f,z)$ is $L$-Lipschitz continuous over $f$, that is
\begin{align*}
  \forall f, f\in \mathcal{H}, z\in\mathcal{Z}, |\nu(f,z)-\nu(f',z)|\leq L\|f-f'\|,
\end{align*}
it is easy to verity that
\begin{align}
  \nonumber R(f)-R(f_\ast)&\leq L\mathbb{E}\left[\|\bar{f}-f_\ast\|\right]\leq L\sqrt{\mathbb{E}\left[\|\bar{f}-f_\ast\|^2\right]}
  %\label{related-work-one}    &= \mathcal{O}\left(\frac{1}{\sqrt{N}}+\frac{\sqrt{\log d}}{n}\right).
\end{align}
Thus, the order of \cite{zhang2012communication,Zhang2013,lin2017distributed} is at most $\mathcal{O}\left(\frac{1}{\sqrt{N}}\right)$.

According to the subsections \label{subsection-3.1} and \label{subsection-3.2},
we know that if $m$ is not very large,
the order of this paper can faster than $\mathcal{O}\left(\frac{1}{N}-\Delta(\bar{f})\right)$,
which is much sharper than the order of the related work \cite{zhang2012communication,Zhang2013,lin2017distributed}.
\section{Max-Discrepant Distributed Learning (MDD)}
In this section, we will propose two novel algorithms based on the finding of the above section.
From corollary \ref{corollary-first}, under some assumptions, we know that
  \begin{align*}
     R(f)-R(f_\ast)=\mathcal{O}\left(\frac{1}{n^2}-\frac{1}{m^2}\sum_{i,j=1,i\not=j}^m\|\hat{f}_i-\hat{f}_j\|_\mathcal{H}^2\right).
\end{align*}
Thus, to obtain tight bound, the discrepancy of each local estimate $\hat{f}_i, i=1,\ldots,m$ should be large.
In the next, we will propose two algorithms for linear space and RKHS.
\subsection{Linear Hypothesis Space}
\begin{algorithm}[t]
    \caption{Max-Discrepant Distributed Learning (MDD)}
    \label{alg:RMMKL}
    \begin{algorithmic}[1]
    \STATE \textbf{Input}: $\lambda,\gamma$, $\mathbf X$, $m$, $\zeta>0$.
    \STATE For each branch node $i$:  $\hat{\mathbf w}_i^0=\mathbf A_i^{-1} \mathbf b_i$ and push $\hat{\mathbf w}_i^t$ to center node;
    \STATE Center node: $\bar{\mathbf w}^0=\frac{1}{m}\sum_{i=1}^m\hat{\mathbf w}_0^t$ and
    push $\bar{\mathbf w}^{0}_{\backslash i}=\frac{m\bar{\mathbf w}^{t}-\hat{\mathbf w}_i^0}{m-1}$ to each branch node $i$
    \FOR{$t=1,2,\ldots$}
    \STATE  For each branch node $i$:
    \STATE ~~~~$\mathbf d_i^t=\frac{\left(\bar{\mathbf w}^0_{\backslash i}\right)^\mathrm{T}\hat{\mathbf w}_i^0}{\mathbf b_i}$,
     $\hat{\mathbf w}_i^t=\hat{\mathbf w}_i^0-\gamma\mathbf d_i^t$;
    \STATE ~~~~push $\hat{\mathbf w}_i^t$ to center node;
     \STATE Center node:
     \STATE ~~~~$\bar{\mathbf w}^t=\frac{1}{m}\sum_{i=1}^m\hat{\mathbf w}_i^t$\\
      ~~~~\textbf{if} {$\|\bar{\mathbf w}^{t}-\bar{\mathbf w}^{t-1}\|\leq \zeta$} \textbf{end for}
      \STATE \textbf{else}
      \STATE ~~~~push $\bar{\mathbf w}^{t}_{\backslash i}=\frac{m\bar{\mathbf w}^{t}-\hat{\mathbf w}_i^t}{m-1}$ to each branch node $i$
    \ENDFOR
    \STATE \textbf{Output}: $\bar{\mathbf w}=\frac{1}{m}\sum_{i=1}^m\hat{\mathbf w}_i^t$
    \end{algorithmic}
\end{algorithm}
When $\mathcal{H}$ is a linear Hypothesis space,
we consider the following optimization problem:
\begin{align}
 \label{optimation-linear-space}
  \hat{\mathbf w}_i=\argmin_{\mathbf w\in\mathbb{R}^d}
  \frac{1}{n}\sum_{z_i\in\mathcal{S}_i}(\mathbf w^\mathrm{T}\mathbf x_i-y_i)^2+\lambda \|\mathbf w\|_2^2-\gamma \|\mathbf w-\bar{\mathbf w}_{\backslash i}\|_2^2,
\end{align}
where $\bar{\mathbf w}_{\backslash i}=\frac{1}{m-1}\sum_{j=1,j\not =i}\hat{\mathbf w}_j$
is used to make the discrepancy of each local estimate large.
Note that, if given $\bar{\mathbf w}_{\backslash i}$,  $\hat{\mathbf w}_i$ can be written as
\begin{align*}
  \hat{\mathbf w}_i=\left(\frac{1}{n}\mathbf X_{\mathcal{S}_i}\mathbf X_{\mathcal{S}_i}^\mathrm{T}+\lambda \mathbf I_d-\gamma \mathbf I_d\right)^{-1}
  \left(\frac{1}{n}\mathbf X_{\mathcal{S}_i}\mathbf y_{\mathcal{S}_i}-\gamma \bar{\mathbf w}_{\backslash i}\right),
\end{align*}
where $\mathbf X_{\mathcal{S}_i}=(\mathbf x_{t_1},\mathbf x_{t_2},\ldots, \mathbf x_{t_n})$,
$\mathbf y_{\mathcal{S}_i}=(y_{t_1},y_{t_2},\ldots,y_{t_n})^\mathrm{T}$, $z_{t_j}\in \mathcal{S}_i$, $j=1,\ldots, n$.
In the next, we will give a iterative algorithm to
solve the optimization problem \ref{optimation-linear-space},
but in each iterative, we should compute $\mathbf A_i^{-1}\bar{\mathbf w}_{\backslash i}$, which is computationally intensive,
where $\mathbf A_i=\frac{1}{n}\mathbf X_{\mathcal{S}_i}\mathbf X_{\mathcal{S}_i}^\mathrm{T}+
\lambda \mathbf I_d-\gamma \mathbf I_d$.
\begin{lemma}
  \label{lemma-fast-linear-space}
  If $\mathbf A\in\mathbb{R}^{l\times l}$ is a symmetric matrix and $\mathbf c=\mathbf A^{-1}\mathbf b\in\mathbb{R}^l$,
  then we have
  \begin{align*}
  \mathbf A^{-1}\mathbf d=(\mathbf d^\mathrm{T}\mathbf c)./\mathbf c,
  \end{align*}
  where $a./\mathbf c=(a/c_1,\ldots a/c_l)^\mathrm{T}$.
\end{lemma}
\begin{proof}
Since $\mathbf A$ a symmetric matrix,
we have
  \begin{align*}
    \left(\mathbf A^{-1}\mathbf d\right)^\mathrm{T}\mathbf b=\mathbf d^\mathrm{T}\mathbf A^{-1}\mathbf b=\mathbf d^\mathrm{T}\mathbf c.
  \end{align*}
Therefore, we can obtain that $\mathbf A^{-1}\mathbf d=(\mathbf d^\mathrm{T}\mathbf c)./\mathbf c$.
\end{proof}
From Lemma  \ref{lemma-fast-linear-space}, let $\mathbf b_i=\frac{1}{n}\mathbf X_{\mathcal{S}_i}\mathbf y_{\mathcal{S}_i}$, $\mathbf c_i=\mathbf A_i^{-1}\mathbf b_i$ we know that
\begin{align*}
  \mathbf A_i^{-1}\bar{\mathbf w}_{\backslash i}=
  \left(\bar{\mathbf w}_{\backslash i}^\mathrm{T}\mathbf c_i\right)./\mathbf c_i,
\end{align*}
which only need $\mathcal{O}(d)$.
\subsection{Reproducing Kernel Hilbert Space}
When $\mathcal{H}$ is a reproducing kernel Hilbert space, that is $f(\mathbf x)=\sum_{j=1}^n c_j K(\mathbf x_j,\mathbf x)$,
we consider the following optimization problem:
\begin{align}
\label{opti-RKHS}
  \hat{\mathbf c}_i=\argmin_{\mathbf c\in\mathbb{R}^n}
  \frac{1}{n}\|\mathbf K_{\mathcal{S}_i}\mathbf c-\mathbf y_{\mathcal{S}_i}\|_2^2+\lambda \mathbf c^\mathrm{T}\mathbf K_{\mathcal{S}_i}\mathbf c
  -\gamma \left(\mathbf c- \bar{\mathbf c}_{\backslash i}\right)^\mathrm{T}
  \mathbf K_{\mathcal{S}_i}\left(\mathbf c- \bar{\mathbf c}_{\backslash i}\right),
\end{align}
where $\bar{\mathbf c}_{\backslash i}=\frac{1}{m-1}\sum_{j=1,j\not =i}\hat{\mathbf c}_j$.
If given $\bar{\mathbf c}_{\backslash i}$, $\hat{\mathbf c}_i$ can be written as
\begin{align*}
  \hat{\mathbf c}_i=\left(\mathbf K_{\mathcal{S}_i}+\lambda \mathbf I_n-\gamma\mathbf I_n\right)^{-1}
  \left(\mathbf y_{\mathcal{S}_i}-\gamma \bar{\mathbf c}_{\backslash i}\right).
\end{align*}
Let $\mathbf A_i=\mathbf K_{\mathcal{S}_i}+\lambda \mathbf I_n-\gamma\mathbf I_n$, $\mathbf b_i=\mathbf y_{\mathcal{S}_i}$.
\begin{algorithm}[t]
    \caption{Max-Discrepant Distributed Learning for RKHS (MDD-RKHS)}
    \label{alg:RMMKL}
    \begin{algorithmic}[1]
    \STATE \textbf{Input}: $\lambda,\gamma$, $\mathbf X$, $m$, $\zeta>0$.
    \STATE For each branch node $i$:  $\hat{\mathbf c}_i^0=\mathbf A_i^{-1} \mathbf b_i$ and push $\hat{\mathbf c}_i^0$ to center node;
    \STATE Center node: $\bar{\mathbf c}^0=\frac{1}{m}\sum_{i=1}^m\hat{\mathbf c}_0^t$ and
    push $\bar{\mathbf c}^{0}_{\backslash i}=\frac{m\bar{\mathbf c}^{t}-\hat{\mathbf c}_i^0}{m-1}$ to each branch node $i$
    \FOR{$t=1,2,\ldots$}
    \STATE  For each branch node $i$:
    \STATE ~~~~$\mathbf d_i^t=\frac{\left(\bar{\mathbf c}^0_{\backslash i}\right)^\mathrm{T}\hat{\mathbf c}_i^0}{\mathbf b_i}$,
     $\hat{\mathbf w}_i^t=\hat{\mathbf c}_i^0-\gamma\mathbf d_i^t$;
    \STATE ~~~~push $\hat{\mathbf c}_i^t$ to center node;
     \STATE Center node:
     \STATE ~~~~$\bar{\mathbf c}^t=\frac{1}{m}\sum_{i=1}^m\hat{\mathbf c}_i^t$\\
      ~~~~\textbf{if} {$\|\bar{\mathbf c}^{t}-\bar{\mathbf c}^{t-1}\|\leq \zeta$} \textbf{end for}
      \STATE \textbf{else}
      \STATE ~~~~push $\bar{\mathbf c}^{t}_{\backslash i}=\frac{m\bar{\mathbf w}^{t}-\hat{\mathbf c}_i^t}{m-1}$ to each branch node $i$
    \ENDFOR
    \STATE \textbf{Output}: $\bar{\mathbf c}=\frac{1}{m}\sum_{i=1}^m\hat{\mathbf c}_i^t$
    \end{algorithmic}
\end{algorithm}
\subsection{Complexity}
Linear space:  for each node, we need $\min \mathcal{O}\left(\{nd^2,n^2d\}\right)$ to compute the $\mathbf A_i$, and $\mathcal{O}(d^3)$ 
to compute $\mathbf A_i^{-1}$,  and need $\mathcal{O}(d)$ for each iterative,
the communication complexity is $O(d)$ for each iterative.
So, the total complexity is $\mathcal{O}\left(\{mnd^2,mn^2d\}+md^3+Tmd\right)$, where $T$ is the number of iterative.

RKHS: we need $\min \mathcal{O}\left(n^2d\right)$ to compute the $\mathbf A_i$, and $\mathcal{O}(n^3)$
to compute $\mathbf A_i^{-1}$,  and need $\mathcal{O}(n)$ for each iterative,
the communication complexity is $O(n)$ for each iterative.
So, the total complexity is $\mathcal{O}\left(mn^2d+mn^3+Tmn\right)$.  

For the average mixture algorithm, for linear space, the complexity is $\mathcal{O}\left(\{mnd^2,mn^2d\}+md^3\right)$,
for RKHS, the complexity is $\mathcal{O}\left(mn^2d+mn^3\right)$.
\section{Analysis}
\subsection{The Key Idea}
Since $\nu(f,z)$ is a $\eta$-strongly convex function,
so both the risk $R(f)=\mathbb{E}_{z\sim\mathbb{P}}\nu(f,z)$ and empirical risk $\hat{R}(f)=\frac{1}{n}\sum_{i=1}^n\nu(f,z_i)$
are $\eta$-strongly convex functions.
%Since $R(f)$ is $\eta$-strongly convex function,
%we have
By \eqref{assumption-strongly-second}, we can obtain that
\begin{align*}
  R(\bar{f})=R\left(\frac{1}{m}\sum_{i=1}^m\hat{f}_i\right)\leq
  \frac{1}{m}\sum_{i=1}^mR(\hat{f}_i)-\frac{\eta}{4m^2}\sum_{i,j=1, i\not=j}^m\|\hat{f}_i-\hat{f}_j\|_\mathcal{H}^2.
\end{align*}
Therefore, we have
\begin{align}
\label{equaiton-strongly-ff}
  R(\bar{f})-R(f_\ast)\leq \frac{1}{m}\sum_{i=1}^m\left[R(\hat{f}_i)-R(f_\ast)\right]
  -\frac{\eta}{4m^2}\sum_{i,j=1,i\not=j}^m\|\hat{f}_i-\hat{f}_j\|_\mathcal{H}^2.
\end{align}

In the next, we will estimate $R(\hat{f}_i)-R(f_\ast)$,
which is built upon the following inequality from \eqref{assumption-strongly-equation}:
\begin{align}
\nonumber
  &~~~~~R(\hat{f}_i)-R(f_\ast)+\frac{\eta}{2}\|\hat{f}_i-f_\ast\|_\mathcal{H}^2
  \leq \langle \nabla R(\hat{f}_i),\hat{f}_i-f_\ast\rangle_\mathcal{H}
  \\ \nonumber &=\langle \nabla R(\hat{f}_i)-\nabla R(f_\ast)-[\nabla \hat{R}_i(\hat{f}_i)-\nabla \hat{R}_i(f_\ast)],
   \hat{f}_i-f_\ast\rangle_\mathcal{H}\\
   \label{equaiton-ddd}
   &~~~~+\langle\nabla R(f_\ast)-\nabla \hat{R}_i(f_\ast), \hat{f}_i-f_\ast\rangle_\mathcal{H}+\langle \nabla \hat{R}_i(\hat{f}_i), \hat{f}_i-f_\ast\rangle_\mathcal{H}.
   %&\nonumber\leq\langle \nabla R(\hat{f}_i)-\nabla R(f_\ast)-[\nabla \hat{R}_i(\hat{f}_i)-\nabla \hat{R}_i(f_\ast)],
%   \hat{f}_i-f_\ast\rangle_\mathcal{H}+\langle \nabla R(f_\ast)-\nabla \hat{R}_i(f_\ast), \hat{f}_i-f_\ast\rangle_\mathcal{H}\\
%   \label{equation-important-middle}
%   &\leq \left(\underbrace{\left\|\nabla R(\hat{f}_i)-\nabla R(f_\ast)-[\nabla \hat{R}_i(\hat{f}_i)-\nabla \hat{R}_i(f_\ast)]\right\|}_{:=A_1}+
%   \underbrace{\left\|\nabla R(f_\ast)-\nabla \hat{R}_i(f_\ast)\right\|}_{:A_2}\right)\left\|\hat{f}_i-f_\ast \right\|
\end{align}
By the convexity of $\hat{R}_i(\cdot)$ and the optimality condition of $\hat{f}_i$ \cite{boyd2004convex},
we have
\begin{align}
  \label{optimal-empirical}
  \langle \nabla \hat{R}_i(\hat{f}_i),f-\hat{f}_i\rangle_\mathcal{H}\geq 0, \forall f\in\mathcal{H}.
\end{align}
Substituting \eqref{optimal-empirical} into \eqref{equaiton-ddd}, we have
\begin{align}
  \nonumber
  &~~~~~R(\hat{f}_i)-R(f_\ast)+\frac{\eta}{2}\|\hat{f}_i-f_\ast\|_\mathcal{H}^2\\
  &\nonumber\leq \langle \nabla R(\hat{f}_i)-\nabla R(f_\ast)-[\nabla \hat{R}_i(\hat{f}_i)-\nabla \hat{R}_i(f_\ast)],
   \hat{f}_i-f_\ast\rangle_\mathcal{H}+\langle\nabla R(f_\ast)-\nabla \hat{R}_i(f_\ast), \hat{f}_i-f_\ast\rangle_\mathcal{H}\\
   \label{equation-important-middle}
   &\leq \left(\underbrace{\left\|\nabla R(\hat{f}_i)-\nabla R(f_\ast)-[\nabla \hat{R}_i(\hat{f}_i)-\nabla \hat{R}_i(f_\ast)]\right\|}_{:=A_1}+
   \underbrace{\left\|\nabla R(f_\ast)-\nabla \hat{R}_i(f_\ast)\right\|}_{=:A_2}\right)\left\|\hat{f}_i-f_\ast \right\|
\end{align}

%where $\eta$ is the strong convexity modulus of $R(\cdot)$ if exists otherwise it is zero.
\begin{lemma}[Seen in Appendix]
\label{lemma-nablaR-nablahat}
  Under \textbf{Assumptions} \ref{assumption-smooth}, with probability at least $1-\delta$,
  for any $f\in\mathcal{N}(\mathcal{H},\epsilon)$,
  we have
  \begin{align}
  \label{equation-nabalRR-empRR}
  \begin{aligned}
 % \nonumber
    &~~~~\left\|\nabla R(f)-\nabla R(f_\ast)-[\nabla \hat{R}_i(f)-\nabla \hat{R}_i(f_\ast)]\right\| \\
   &\leq \frac{\beta \log C(\mathcal{H},\epsilon)\|f-f_\ast\|}{n}+\sqrt{\frac{\beta \log C(\mathcal{H},\epsilon)(R(f)-R(f_\ast))}{n}}.
  \end{aligned}
  \end{align}
\end{lemma}


\begin{lemma}[Seen in Appendix]
\label{lemma-second-ff}
 Under \textbf{Assumptions} \ref{assumption-smooth} and \ref{assumption-optimal-bound},
 with probability at least $1-\delta$, we have
  \begin{align}
  \label{equation-nablaR-nablaempR}
    \left\|\nabla R(f_\ast)-\nabla \hat{R}_i(f_\ast)\right\|\leq \frac{2M\log(2/\delta)}{n}+\sqrt{\frac{8\beta R_\ast\log(2/\delta)}{n}}.
  \end{align}
\end{lemma}


%\begin{theorem}
%  At least $1-2\delta$,
%  if
%  \begin{align}
%    \label{equation-12}
%    n\geq \frac{4\beta\log C(\mathcal{H},\epsilon)}{\eta},
%  \end{align}
%  we have
%  \begin{align}
%    \label{equation-13}
%    \begin{aligned}
%    R(\bar{f})-R(f_\ast)&\leq
%    \frac{16\beta \log(2m/\delta)}{n^2\eta}+\frac{128\beta R_\ast\log(2m/\delta)}{n\eta}+\frac{32\beta^2\epsilon^2}{\eta}\\
%   &~~~~+
%   \frac{64\beta L \log C(\mathcal{H},\epsilon)\epsilon}{n\eta}+
%   \frac{64\beta \log^2 C(\mathcal{H},\epsilon)\epsilon^2}{n^2\eta}\\
%  &~~~~-\frac{\eta}{4m^2}\sum_{i,j=1,i\not=j}^m\|\hat{f}_i-\hat{f}_j\|_\mathcal{H}^2.
%  \end{aligned}
%  \end{align}
%\end{theorem}

\begin{proof}[Proof of Theorem \ref{theorem-main}]
From the property of $\epsilon$-net, we know that there exists a point
$\tilde{f}\in\mathcal{N}(\mathcal{H},\epsilon)$ such that $$\|\hat{f}_i-\tilde{f}\|\leq \epsilon.$$
According to \textbf{Assumption} \ref{assumption-smooth}, we have
\begin{align}
  \nonumber
    &~~~~~\left\|\nabla R(\hat{f}_i)-\nabla R(f_\ast)-[\nabla \hat{R}_i(\hat{f}_i)-\nabla \hat{R}_i(f_\ast)]\right\| \\
   \nonumber &\leq \left\|\nabla R(\tilde{f})-\nabla R(f_\ast)-[\nabla \hat{R}_i(\tilde{f})-\nabla \hat{R}_i(f_\ast)]\right\|+2\beta \epsilon\\
   \nonumber &\overset{\eqref{equation-nabalRR-empRR}}{\leq}
   \frac{\beta \log C(\mathcal{H},\epsilon)\|\tilde{f}-f_\ast\|}{n}+
   \sqrt{\frac{\beta \log C(\mathcal{H},\epsilon)(R(\tilde{f})-R(f_\ast))}{n}}+2\beta\epsilon\\
   \nonumber&\leq \frac{\beta \log C(\mathcal{H},\epsilon)\|\hat{f}_i-f_\ast\|_\mathcal{H}}{n}
   +\frac{\beta \log C(\mathcal{H},\epsilon)\epsilon}{n}+2\beta\epsilon\\
   \nonumber&~~~~~+\sqrt{\frac{\beta \log C(\mathcal{H},\epsilon)(R(\hat{f}_i)-R(f_\ast))}{n}}+
   \sqrt{\frac{\beta \log C(\mathcal{H},\epsilon)\left(\left|R(\hat{f}_i)-R(\tilde{f})\right|\right)}{n}}\\
   \nonumber &\overset{\eqref{assumption-libs-equation}}{\leq} \frac{\beta \log C(\mathcal{H},\epsilon)\|\hat{f}_i-f_\ast\|_\mathcal{H}}{n}+
   \frac{\beta \log C(\mathcal{H},\epsilon)\epsilon}{n}+2\beta\epsilon\\
   \label{equation-31}&~~~~+\sqrt{\frac{\beta \log C(\mathcal{H},\epsilon)(R(\hat{f}_i)-R(f_\ast))}{n}}+
   \sqrt{\frac{\beta L \log C(\mathcal{H},\epsilon)\epsilon}{n}}
\end{align}

  Substituting \eqref{equation-31} and \eqref{equation-nablaR-nablaempR} into \eqref{equation-important-middle},
  with probability at least $1-2\delta$, we have
  \begin{align}
    \label{equation-first-equation}
    \begin{aligned}
      &R(\hat{f}_i)-R(f_\ast)+\frac{\eta}{2}\|\hat{f}_i-f_\ast\|_\mathcal{H}^2\\
      &\leq \frac{\beta \log C(\mathcal{H},\epsilon)\|\hat{f}_i-f_\ast\|_\mathcal{H}^2}{n}+
   \frac{\beta \log C(\mathcal{H},\epsilon)\epsilon \|\hat{f}_i-f_\ast\|_\mathcal{H}}{n}+2\beta\epsilon \|\hat{f}_i-f_\ast\|_\mathcal{H}\\
   &~~~~+\|\hat{f}_i-f_\ast\|_\mathcal{H}\sqrt{\frac{\beta \log C(\mathcal{H},\epsilon)(R(\hat{f}_i)-R(f_\ast))}{n}}+
   \|\hat{f}_i-f_\ast\|_\mathcal{H}\sqrt{\frac{\beta L \log C(\mathcal{H},\epsilon)\epsilon}{n}}\\
   &~~~~+\frac{2M \log(2/\delta)\|\hat{f}_i-f_\ast\|_\mathcal{H}}{n}+\|\hat{f}_i-f_\ast\|_\mathcal{H}\sqrt{\frac{8\beta R_\ast \log(2/\delta)}{n}}.
    \end{aligned}
  \end{align}
  Note that
  \begin{align*}
    \sqrt{ab}\leq \frac{a}{2c}+\frac{bc}{2}, \forall a,b,c\geq 0.
  \end{align*}
  Therefore, we can obtain that
  \begin{align*}
  %\label{equation-finally}
%  \begin{aligned}
    \|\hat{f}_i-f_\ast\|_\mathcal{H}\sqrt{\frac{\beta \log C(\mathcal{H},\epsilon)(R(\hat{f}_i)-R(f_\ast))}{n}}
    &\leq \frac{2\beta \log C(\mathcal{H},\epsilon)(R(\hat{f}_i)-R(f_\ast))}{n\eta}+\frac{\eta }{8}\|\hat{f}_i-f_\ast\|_\mathcal{H}^2,\\
    \frac{2M \log(2/\delta)\|\hat{f}_i-f_\ast\|_\mathcal{H}}{n}&\leq \frac{8M \log(2/\delta)}{n^2\eta}+\frac{\eta }{16}\|\hat{f}_i-f_\ast\|_\mathcal{H}^2,\\
    \|\hat{f}_i-f_\ast\|_\mathcal{H}\sqrt{\frac{8\beta R_\ast \log(2/\delta)}{n}}&\leq \frac{64\beta R_\ast\log(2/\delta)}{n\eta}+\frac{\eta }{32}\|\hat{f}_i-f_\ast\|_\mathcal{H}^2,\\
    2\beta\epsilon \|\hat{f}_i-f_\ast\|_\mathcal{H}&\leq \frac{32\beta^2\epsilon^2}{\eta}+\frac{\eta }{64}\|\hat{f}_i-f_\ast\|_\mathcal{H}^2,\\
    \|\hat{f}_i-f_\ast\|_\mathcal{H}\sqrt{\frac{\beta L \log C(\mathcal{H},\epsilon)\epsilon}{n}}&
    \leq \frac{32\beta L \log C(\mathcal{H},\epsilon)\epsilon}{n\eta}+\frac{\eta }{128}\|\hat{f}_i-f_\ast\|_\mathcal{H}^2\\
    \frac{\beta \log C(\mathcal{H},\epsilon)\epsilon \|\hat{f}_i-f_\ast\|_\mathcal{H}}{n}
    &\leq \frac{32\beta \log^2 C(\mathcal{H},\epsilon)\epsilon^2}{n^2\eta}+\frac{\eta }{128}\|\hat{f}_i-f_\ast\|_\mathcal{H}^2.
  %\end{aligned}
  \end{align*}
 Substituting the above  inequation into \eqref{equation-first-equation}, we can obtain that
 \begin{align*}
   &~~~~R(\hat{f}_i)-R(f_\ast)+\frac{\eta}{4}\|\hat{f}_i-f_\ast\|_\mathcal{H}^2\\
   &\leq \frac{\beta \log C(\mathcal{H},\epsilon)\|\hat{f}_i-f_\ast\|_\mathcal{H}^2}{n}+
   \frac{2\beta \log C(\mathcal{H},\epsilon)(R(\hat{f}_i)-R(f_\ast))}{n\eta}+\frac{8M \log(2/\delta)}{n^2\eta}\\
   &~~~+\frac{64\beta R_\ast\log(2/\delta)}{n\eta}+\frac{32\beta^2\epsilon^2}{\eta}+
   \frac{32\beta L \log C(\mathcal{H},\epsilon)\epsilon}{n\eta}+
   \frac{32\beta \log^2 C(\mathcal{H},\epsilon)\epsilon^2}{n^2\eta}\\
   &\overset{\eqref{equation-12}}{\leq}
   \frac{\eta}{4}\|\hat{f}_i-f_\ast\|_\mathcal{H}^2+\frac{1}{2}(R(\hat{f}_i)-R(f_\ast))+\frac{8\beta \log(2/\delta)}{n^2\eta}\\
   &~~~+\frac{64\beta R_\ast\log(2/\delta)}{n\eta}+\frac{32\beta^2\epsilon^2}{\eta}+
   \frac{32\beta L \log C(\mathcal{H},\epsilon)\epsilon}{n\eta}+
   \frac{32\beta \log^2 C(\mathcal{H},\epsilon)\epsilon^2}{n^2\eta}.
 \end{align*}
 Thus, with $1-2\delta$, we have
 \begin{align}
    \label{equation-14}
    \begin{aligned}
    R(\hat{f}_i)-R(f_\ast)&\leq
    \frac{16M \log(2/\delta)}{n^2\eta}+\frac{128\beta R_\ast\log(2/\delta)}{n\eta}+\frac{32\beta^2\epsilon^2}{\eta}\\
    &~~~~+
   \frac{64\beta L \log C(\mathcal{H},\epsilon)\epsilon}{n\eta}+
   \frac{64\beta \log^2 C(\mathcal{H},\epsilon)\epsilon^2}{n^2\eta}.
   \end{aligned}
  \end{align}
  Combining \eqref{equaiton-strongly-ff} and \eqref{equation-14},
  with $1-\delta$,
  we have
  \begin{align*}
    R(\bar{f})-R(f_\ast)&\leq
    \frac{16M \log(4m/\delta)}{n^2\eta}+\frac{128\beta R_\ast\log(4m/\delta)}{n\eta}+\frac{32\beta^2\epsilon^2}{\eta}+
   \frac{64\beta L \log C(\mathcal{H},\epsilon)\epsilon}{n\eta}+\\
   &~~~~+
   \frac{64\beta \log^2 C(\mathcal{H},\epsilon)\epsilon^2}{n^2\eta}
  -\frac{\eta}{4m^2}\sum_{i,j=1,i\not=j}^m\|\hat{f}_i-\hat{f}_j\|_\mathcal{H}^2.
  \end{align*}
\end{proof}

%\begin{theorem}
%  At least $1-2\delta$,
%  if
%  \begin{align}
%    \label{equation-12}
%    n\geq \frac{4\beta\log C(\mathcal{H},\epsilon)}{\eta},
%  \end{align}
%  we have
%  \begin{align}
%    \label{equation-13}
%    \begin{aligned}
%    R(\hat{f}_i)-R(f_\ast)&\leq
%    \frac{16\beta \log(2/\delta)}{n^2\eta}+\frac{128\beta R_\ast\log(2/\delta)}{n\eta}+\frac{32\beta^2\epsilon^2}{\eta}\\
%    &~~~~+
%   \frac{64\beta L \log C(\mathcal{H},\epsilon)\epsilon}{n\eta}+
%   \frac{64\beta \log^2 C(\mathcal{H},\epsilon)\epsilon^2}{n^2\eta}
%   \end{aligned}
%  \end{align}
%\end{theorem}
%\section*{}
\subsection*{Appendix: Proof of Lemma \ref{lemma-nablaR-nablahat}}
\begin{lemma}[\cite{smale2007learning}]
  \label{lem-nonequation}
    Let $\mathcal{H}$ be a Hilbert space and let $\xi$ be a random variable with values in $\mathcal{H}$.
    Assume $\|\xi\|\leq M\leq \infty$ almost surely.
    Denote $\sigma^2(\xi)=\mathbb{E}[\|\xi\|^2]$.
    Let $\{\xi_i\}_{i=1}^n$ be $m$ independent drawers of $\xi$.
    For any $0\leq \delta\leq 1$, with confidence $1-\delta$,
    \begin{align*}
      \left\|\frac{1}{n}\sum_{j=1}^n[\xi_j-\mathbb{E}[\xi_j]]\right\|\leq \frac{2M\log(2/\delta)}{n}+\sqrt{\frac{2\sigma^2(\xi)\log(2/\delta)}{n}}.
    \end{align*}
  \end{lemma}
\begin{proof}
  Note that $\nu(f,\cdot)$ is $\beta$-smooth,
  so we have
  \begin{align}
    \|\nabla\nu(f,\cdot)-\nabla\nu(f_\ast,\cdot)\|_\mathcal{H} \leq \beta\|f-f_\ast\|_\mathcal{H}
  \end{align}
  Because $\nu(f,\cdot)$ is $\beta$-smooth and convex, by (2.1.7) of \cite{Nesterov2004},
  $\forall z\in\mathcal{Z}$,
  we have
  \begin{align*}
   \left\|\nabla\nu (f,z)-\nabla \nu (f_\ast,z)\right\|^2
    \leq
    \beta\left(\nu (f,z)-\nu (f_\ast,z)
    -\langle \nabla\nu (f_\ast,z), f-f_\ast\rangle_\mathcal{H} \right).
  \end{align*}
  Taking expectation over both sides, we have
  \begin{align*}
    &~~~~\mathbb{E}_{z\sim\mathbb{P}}[\left\|\nabla\nu (f,\cdot)-\nabla\nu (f_\ast,\cdot)\right\|^2]\\
    &\leq \beta \left(R(\hat{f}_i)-R(f_\ast)-\langle \nabla R(f_\ast), f-f_\ast\rangle_\mathcal{H}\right)\\
    &\leq \beta \left(R(\hat{f}_i)-R(f_\ast)\right)
  \end{align*}
  where the last inequality follows from the optimality condition of $f_\ast$, i.e.,
  \begin{align*}
    \langle \nabla R(f_\ast),f-f_\ast\rangle_\mathcal{H} \geq 0,\forall f\in\mathcal{H}.
  \end{align*}

Following Lemma \ref{lem-nonequation}, with probability at least $1-\delta$, we have
\begin{align*}
  &~~~~\left\|
  \nabla R(f)-\nabla R(f_\ast)-[\nabla \hat{R}_i(f)-\nabla \hat{R}_i(f_\ast)]
  \right\|_\mathcal{H}\\
  &=\left\|
    \nabla R(f)-\nabla R(f_\ast)
    -\frac{1}{n}\sum_{z_i\in \mathcal{S}_i}
    \left[\nabla \nu(f,z_i)-\nabla \nu(f_\ast,z_i)\right]
  \right\|_\mathcal{H}\\
  &\leq \frac{2\beta\|f-f_\ast\|_\mathcal{H}\log(2/\delta)}{n}+\sqrt{\frac{2\beta(R(f)-R(f_\ast))\log (2/\delta)}{n}}
\end{align*}

We obtain Lemma \ref{lemma-nablaR-nablahat} by taking the union bound over all $f\in\mathcal{N}(\mathcal{H},\epsilon)$.
\end{proof}
\subsection{Appendix: Proof of Lemma \ref{lemma-second-ff}}
\begin{proof}
  Since $\nu(f,z_i)$ is $\beta$-smooth and nonegative,
  from Lemma 4 of \cite{srebro2010optimistic}, we have
  \begin{align*}
    \left\|\nabla \nu(f_\ast,z_i)\right\|^2\leq 4\beta \nu(f_\ast,z_i)
  \end{align*}
  and thus
    \begin{align*}
      \mathbb{E}_{z\sim\mathbb{P}}\left[\left\|\nabla \nu(f_\ast,z)\right\|^2\right]\leq 4\beta\mathbb{E}_{z\sim\mathbb{P}}[\nu(f_\ast,z)]=
      4\beta R(f_\ast).
    \end{align*}
    From the \textbf{Assumption}, we have $\nabla \|\nu(f_\ast, z)\|\leq M$, $\forall z\in\mathcal{Z}$.
    Then, according to Lemma \ref{lem-nonequation}, with probability at least $1-\delta$, we have
    \begin{align*}
      &\left\|\nabla R(f_\ast)-\nabla \hat{R}_i(f_\ast)\right\|=\left\|\nabla R(f_\ast)-\frac{1}{n}\sum_{z_j\in\mathcal{S}_i}\nabla \nu(f_\ast,z_j)\right\|\\
      &\leq \frac{2\beta \log(2/\delta)}{n}+\sqrt{\frac{8\beta R_\ast \log(2/\delta)}{n}}.
    \end{align*}
\end{proof}
\bibliographystyle{abbrv}
\bibliography{NIPS2018}
\end{document}
